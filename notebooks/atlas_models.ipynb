{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from metadata import ImageDataset, patient\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory is: /home/ricardino/Documents/MAIA/tercer_semestre/MISA/final_project/MISA_FINAL_PROJECT/notebooks\n"
     ]
    }
   ],
   "source": [
    "notebooks_path = Path.cwd()\n",
    "repo_path = notebooks_path.parent\n",
    "print(f'The current directory is: {notebooks_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the propagated labels, as well as a metric for the similarity between the registered images.<br>\n",
    "We can now build several versions of the atlas.\n",
    "We start with the most common, the **probabilistic atlas**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic mean atlas\n",
    "\n",
    "- All atlases (labels) are summed up and divided by the number of images. This is basically weighted voting with the same weight for all images (1/n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each patient in the validation, we will accumulate the labels, summing them.\n",
    "im_data_val = ImageDataset(set_name='Validation')\n",
    "im_data_train = ImageDataset(set_name='Training')\n",
    "#dataframe to store probabilistic atlas\n",
    "df_mean_atlas = None\n",
    "\n",
    "for id_val in im_data_val.IDs:\n",
    "    #instantiate patient\n",
    "    pat_val = patient(id_val, im_data_val)\n",
    "    #accumulated label array\n",
    "    accumulated_label = np.zeros((4,) + pat_val.im(format='np').shape)\n",
    "    for id_train in im_data_train.IDs:\n",
    "        moved_label_path = repo_path / 'data'/'voxelmorph'/f'moved_labels_{id_train}_to_{id_val}.nii.gz'\n",
    "        moved_label = utils.getArrayfromPath(moved_label_path)\n",
    "        #accumulate per tissue\n",
    "        for tissue in range(1,4):\n",
    "            accumulated_label[tissue] += (moved_label==tissue)\n",
    "    #divide by the number of labels\n",
    "    accumulated_label /= im_data_train.len\n",
    "\n",
    "    #Now we can take the argmax of the accumulated label to get the final mean atlas label\n",
    "    mean_label = np.argmax(accumulated_label, axis=0)\n",
    "    #compute the metrics\n",
    "    df_metrics = utils.compute_metrics(mean_label, pat_val, id_val)\n",
    "    #concatenate metrics in main df\n",
    "    df_mean_atlas = pd.concat([df_mean_atlas, df_metrics], axis=0)\n",
    "#save csv\n",
    "df_mean_atlas.to_csv(repo_path / 'data'/'results'/f'mean_atlas_metrics.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted atlas\n",
    "\n",
    "- Similar to the previous one, but the weights are not the same for all images. The weights are the similarity metric between the registered images and the target (validation) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define image datasets\n",
    "im_data_val = ImageDataset(set_name='Validation')\n",
    "im_data_train = ImageDataset(set_name='Training')\n",
    "#dataframe to store probabilistic atlas\n",
    "df_weightedAtlas = None\n",
    "\n",
    "for id_val in im_data_val.IDs:\n",
    "    #instantiate patient\n",
    "    pat_val = patient(id_val, im_data_val)\n",
    "    #array to store the weighted atlas\n",
    "    weighted_atlas = np.zeros((4,) + pat_val.im(format='np').shape)\n",
    "\n",
    "    #Compute the sum of similarity metrics for this validation patient\n",
    "    df_mostSimilar = pd.read_csv(repo_path / 'data'/'results'/'most_similar'/ f'most_similar_{id_val}.csv')\n",
    "    #get metric values and sum them up\n",
    "    sigma = df_mostSimilar['metric'].abs().sum()\n",
    "\n",
    "    for id_train in im_data_train.IDs:\n",
    "        moved_label_path = repo_path / 'data'/'voxelmorph'/f'moved_labels_{id_train}_to_{id_val}.nii.gz'\n",
    "        moved_label = utils.getArrayfromPath(moved_label_path)\n",
    "        #get similarity metric value\n",
    "        simMetric = df_mostSimilar[df_mostSimilar['id_train']==int(id_train)]['metric'].abs().values[0]\n",
    "        #accumulate per tissue\n",
    "        for tissue in range(1,4):\n",
    "            weighted_atlas[tissue] += (moved_label==tissue)*(simMetric/sigma) #wieghting by the similarity metric\n",
    "\n",
    "    #get argmax of the weighted atlas\n",
    "    weighted_label = np.argmax(weighted_atlas, axis=0)\n",
    "    #compute the metrics\n",
    "    df_metrics = utils.compute_metrics(weighted_label, pat_val, id_val)\n",
    "    #concatenate in df_maxM\n",
    "    df_weightedAtlas = pd.concat([df_weightedAtlas, df_metrics], axis=0)\n",
    "#save as csv\n",
    "df_weightedAtlas.to_csv(repo_path / 'data'/'results'/f'weighted_labels_metrics.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top atlases\n",
    "- Now only atlases with high similarity to the target image are used. Basically like mean atlas but with a threshold on the similarity metric.\n",
    "    - Additionally, the max number of atlases can be set (3 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each patient in the validation, we will accumulate the labels, summing them.\n",
    "im_data_val = ImageDataset(set_name='Validation')\n",
    "im_data_train = ImageDataset(set_name='Training')\n",
    "#dataframe to store probabilistic atlas\n",
    "df_top_atlases = None\n",
    "for id_val in im_data_val.IDs:\n",
    "    #instantiate patient\n",
    "    pat_val = patient(id_val, im_data_val)\n",
    "    #accumulated label array\n",
    "    accumulated_label = np.zeros((4,) + pat_val.im(format='np').shape)\n",
    "    #we'll go through the top 3 atlases\n",
    "    df_mostSimilar = pd.read_csv(repo_path / 'data'/'results'/'most_similar'/ f'most_similar_{id_val}.csv', dtype={'id_train': str})\n",
    "    #get the top 3 atlases\n",
    "    df_mostSimilar = df_mostSimilar.iloc[:3]\n",
    "    #remove the last row if the metric values differences are greater than 0.05\n",
    "    df_mostSimilar = df_mostSimilar.iloc[:-1] if df_mostSimilar['metric'].diff().abs().max()>0.05 else df_mostSimilar\n",
    "    for id_train in df_mostSimilar['id_train']:\n",
    "        moved_label_path = repo_path / 'data'/'voxelmorph'/f'moved_labels_{id_train}_to_{id_val}.nii.gz'\n",
    "        moved_label = utils.getArrayfromPath(moved_label_path)\n",
    "        #accumulate per tissue\n",
    "        for tissue in range(1,4):\n",
    "            accumulated_label[tissue] += (moved_label==tissue)\n",
    "    #divide by the number of labels\n",
    "    accumulated_label /= len(df_mostSimilar['id_train'])\n",
    "    #save the accumulated label (top probabilistic atlas) as nifti\n",
    "    for tissue in range(3):\n",
    "        filename = str(repo_path / 'data'/'atlas_data'/f'top_prob_atlas_{id_val}_{tissue}.nii.gz')\n",
    "        utils.save_as_nifti(accumulated_label[tissue+1], filename, pat_val.labels_path, dtype=np.float32)\n",
    "\n",
    "    #Now we can take the argmax of the accumulated label to get the final mean atlas label\n",
    "    top_atlases_label = np.argmax(accumulated_label, axis=0)\n",
    "    #compute the metrics\n",
    "    df_metrics = utils.compute_metrics(top_atlases_label, pat_val, id_val)\n",
    "    #concatenate in df_maxM\n",
    "    df_top_atlases = pd.concat([df_top_atlases, df_metrics], axis=0)\n",
    "    \n",
    "    #save segmentation as nifti\n",
    "    filename = str(repo_path / 'data'/'segmentations'/f'top_atlases_{id_val}_seg.nii.gz')\n",
    "    reference_path = pat_val.labels_path\n",
    "    utils.save_as_nifti(top_atlases_label, filename, reference_path)\n",
    "#save as csv\n",
    "df_top_atlases.to_csv(repo_path / 'data'/'results'/f'top_atlases_metrics.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian atlas\n",
    "- Finally we can combine the probabilistic atlas and the tissue model to obtain a combination of bth intensity and spatial information\n",
    "    - The probabilistic atlas that we choose is the top probabilistic atlas (using only the infomation of the top 3 atlases) as we consider it more reliable than the mean atlas.\n",
    "    - The tissue model is the one that we got in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each patient in the validation, we will accumulate the labels, summing them.\n",
    "im_data_val = ImageDataset(set_name='Validation')\n",
    "im_data_train = ImageDataset(set_name='Training')\n",
    "#dataframe to store probabilistic atlas\n",
    "df_bayesian = None\n",
    "\n",
    "for id_val in im_data_val.IDs:\n",
    "    pat_val = patient(id_val, im_data_val)\n",
    "    bayesian_atlas = np.zeros((4,) + pat_val.labels(format='np').shape)\n",
    "    for tissue_num in range(3):\n",
    "        #get paths\n",
    "        TModel_prob_path = repo_path / 'data'/'atlas_data'/f'TModel_prob_{id_val}_{tissue_num}.nii.gz'\n",
    "        prob_atlas_path = repo_path / 'data'/'atlas_data'/f'top_prob_atlas_{id_val}_{tissue_num}.nii.gz'\n",
    "        #get arrays\n",
    "        TModel_prob = utils.getArrayfromPath(TModel_prob_path, dtype=np.float32)\n",
    "        prob_atlas = utils.getArrayfromPath(prob_atlas_path, dtype=np.float32)\n",
    "        #get the bayesian atlas\n",
    "        bayesian_atlas[tissue_num+1] = TModel_prob*prob_atlas\n",
    "    #get argmax to get the final bayesian atlas\n",
    "    bayesian_label = np.argmax(bayesian_atlas, axis=0)\n",
    "    \n",
    "    #compute the metrics\n",
    "    df_metrics = utils.compute_metrics(bayesian_label, pat_val, id_val)\n",
    "    #concatenate in df_bayesian\n",
    "    df_bayesian = pd.concat([df_bayesian, df_metrics], axis=0)\n",
    "    \n",
    "    #save segmentation as nifti\n",
    "    filename = str(repo_path / 'data'/'segmentations'/f'bayesian_{id_val}_seg.nii.gz')\n",
    "    reference_path = pat_val.labels_path\n",
    "    utils.save_as_nifti(bayesian_label, filename, reference_path)\n",
    "#save as csv\n",
    "df_bayesian.to_csv(repo_path / 'data'/'results'/f'bayesian_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misa_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cff243af6c3d0a2893d2e87262cea0e9d750ffc752eaeb95474b08792ecfb50d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
